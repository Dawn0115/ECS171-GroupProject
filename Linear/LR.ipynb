{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def manual_openfe_features(df_raw):\n",
    "    df = df_raw.copy()\n",
    "    freq_shell = df[\"Shell weight\"].value_counts(normalize=True)\n",
    "    freq_whole = df[\"Whole weight\"].value_counts(normalize=True)\n",
    "    freq_feature_shell = df[\"Shell weight\"].map(freq_shell)\n",
    "    freq_feature_whole = df[\"Whole weight\"].map(freq_whole)\n",
    "    \n",
    "    df_manual = pd.DataFrame({\n",
    "        \"f01_Length_div_ShellWeight\": df[\"Length\"] / df[\"Shell weight\"],\n",
    "        \"f02_Whole1_div_ShellWeight\": df[\"Whole weight.1\"] / df[\"Shell weight\"],\n",
    "        \"f03_Diameter_div_ShellWeight\": df[\"Diameter\"] / df[\"Shell weight\"],\n",
    "        \"f05_Length_minus_Shell\": df[\"Length\"] - df[\"Shell weight\"],\n",
    "        \"f07_freq_ShellWeight\": freq_feature_shell,\n",
    "        \"f08_Max_Whole2_Shell\": df[[\"Whole weight.2\", \"Shell weight\"]].max(axis=1),\n",
    "        \"f09_log_Whole_weight\": np.log(df[\"Whole weight\"]),\n",
    "        \"f10_freq_WholeWeight\": freq_feature_whole,\n",
    "        \"f11_Shell_plus_Height\": df[\"Shell weight\"] + df[\"Height\"],\n",
    "    })\n",
    "    return df_manual\n",
    "\n",
    "\n",
    "df = pd.read_csv('train_cleaned_sex_binary.csv')\n",
    "y = df[\"Rings\"]\n",
    "X_raw = df.drop(columns=[\"Rings\", \"id\"], errors=\"ignore\")\n",
    "X_manual = manual_openfe_features(X_raw)\n",
    "X = pd.concat([X_raw.reset_index(drop=True), X_manual], axis=1)\n",
    "X = X.drop(columns=[\"Length\", \"Whole weight\"], errors=\"ignore\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std  = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "def train_gd(X_tr, y_tr, X_te, y_te, lr, epochs, batch_size=None, alpha=0.01):\n",
    "    n_samples, n_features = X_tr.shape\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0.0\n",
    "    history = {'test_mse': [], 'test_r2': [], 'test_rmsle': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if batch_size:\n",
    "            idx = np.random.permutation(n_samples)\n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                end = start + batch_size\n",
    "                xb = X_tr[idx[start:end]]\n",
    "                yb = y_tr[idx[start:end]]\n",
    "                preds = xb.dot(w) + b\n",
    "                err = preds - yb\n",
    "                grad_w = (2/len(yb)) * xb.T.dot(err) + 2 * alpha * w\n",
    "                grad_b = (2/len(yb)) * err.sum()\n",
    "                w -= lr * grad_w\n",
    "                b -= lr * grad_b\n",
    "        else:\n",
    "            preds = X_tr.dot(w) + b\n",
    "            err = preds - y_tr\n",
    "            grad_w = (2/n_samples) * X_tr.T.dot(err) + 2 * alpha * w\n",
    "            grad_b = (2/n_samples) * err.sum()\n",
    "            w -= lr * grad_w\n",
    "            b -= lr * grad_b\n",
    "        \n",
    "        y_pred = X_te.dot(w) + b\n",
    "        history['test_mse'].append(mean_squared_error(y_te, y_pred))\n",
    "        history['test_r2'].append(r2_score(y_te, y_pred))\n",
    "        y_te_clip   = np.maximum(y_te,   0)\n",
    "        y_pred_clip = np.maximum(y_pred, 0)\n",
    "        history['test_rmsle'].append(np.sqrt(mean_squared_log_error(y_te_clip, y_pred_clip)))\n",
    "    \n",
    "    final_y_pred = X_te.dot(w) + b\n",
    "    final_mse    = mean_squared_error(y_te, final_y_pred)\n",
    "    final_r2     = r2_score(y_te, final_y_pred)\n",
    "    final_msle   = mean_squared_log_error(np.maximum(y_te, 0), np.maximum(final_y_pred, 0))\n",
    "    final_rmsle  = np.sqrt(final_msle)\n",
    "\n",
    "    return w, b, history, final_mse, final_r2, final_rmsle\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    ('Batch GD',      {'lr': 0.015, 'batch_size': None, 'alpha': 0.01}),\n",
    "    ('SGD',           {'lr': 0.001, 'batch_size': 1,    'alpha': 0.01}),\n",
    "    ('Mini-batch GD', {'lr': 0.001, 'batch_size': 32,   'alpha': 0.01}),\n",
    "]\n",
    "\n",
    "results    = {}\n",
    "histories  = {}\n",
    "epochs     = 200\n",
    "\n",
    "for name, params in experiments:\n",
    "    print(f\"\\n▶ Training {name}\")\n",
    "    w, b, hist, final_mse, final_r2, final_rmsle = train_gd(\n",
    "        X_train_std, y_train.values,\n",
    "        X_test_std,  y_test.values,\n",
    "        lr=params['lr'], epochs=epochs,\n",
    "        batch_size=params['batch_size'],\n",
    "        alpha=params['alpha']\n",
    "    )\n",
    "    results[name]   = (final_mse, final_r2, final_rmsle)\n",
    "    histories[name] = hist\n",
    "\n",
    "for name, _ in experiments:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(histories[name]['test_mse'])\n",
    "    plt.title(f\"{name} - Test MSE over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nFinal Test Performance:\")\n",
    "for name, (mse, r2, rmsle) in results.items():\n",
    "    print(f\"{name}: MSE = {mse:.4f}, R² = {r2:.4f}, RMSLE = {rmsle:.4f}\")\n",
    "\n",
    "best_model = min(results, key=lambda k: results[k][0])\n",
    "print(f\"\\nBest model based on Test MSE: {best_model}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
